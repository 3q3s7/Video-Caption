{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.预处理——提取视频特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法介绍\n",
    "\n",
    "提取视频特征是视频描述的第一步，其包括以下两个步骤：\n",
    "\n",
    "1.使用ffmpeg从每个视频中提取帧图片\n",
    "\n",
    "2.根据采样帧数采样图片，利用训练好的CNN模型，对每一个图片提取特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法使用\n",
    "extract_feats(params,model,load_image_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数介绍\n",
    "\n",
    "dst: 保存帧图片的文件夹位置\n",
    "\n",
    "output_dir：保存视频特征.npy的文件夹位置\n",
    "\n",
    "video_path：视频所在文件夹路径\n",
    "\n",
    "model：提取特征的CNN模型，实例中选用resnet152\n",
    "\n",
    "n_frame_steps：每个视频采样的帧数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 命令实例\n",
    "\n",
    "本实验中使用的是预训练的 ResNet152 网络对视频画面进行图像特征的提取；\n",
    "\n",
    "该模型网络基于 Image-Net 进行训练，网络层数为 152 层；网络对每一个输入的图片输出一个 2048维的特征向量；\n",
    "\n",
    "在此基础上，对每个视频画面挑选 40 帧，40 帧画面单独输入网络，得到图像特征，最终每个视频得到一个尺寸为 [40, 2048] 的视频图像特征张量；\n",
    "\n",
    "为节省时间方便演示，提取特征的视频数目为70个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需要的模块\n",
    "import shutil\n",
    "import subprocess\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pretrainedmodels\n",
    "from pretrainedmodels import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "C, H, W = 3, 224, 224 # 特征提取模型所要求的每个视频帧的维度，由于采用ResNet152提取特征，故将C、H、W设置为3、244、244"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从视频中提取帧图片"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入视频，得到视频帧图片，帧图片暂时保存在dst文件夹中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **题1：请补全提取帧图片的代码**  \n",
    "（1）要求实现的功能为：  \n",
    "> - 判断dst文件夹是否存在。若存在，删除dst文件夹中的所有文件；否则，创建dst文件夹  \n",
    "> - 提取输入视频`video`的帧图片，将这些图片保存到`dst`文件夹中  \n",
    "    \n",
    "（2）提示：使用ffmpeg提取视频中的图片。\n",
    "> 在命令行中执行以下指令，可提取视频的帧图片：\n",
    "> > `ffmpeg -y -i ${视频路径} -vf scale=400:300 -qscale:v 2 ${输出图片的路径及命名格式}`  \n",
    "\n",
    "> 示例：  \n",
    "    > >`ffmpeg -y -i test.mp4 -vf scale=400:300 -qscale:v 2 frames/%06d.jpg`  \n",
    "> 在命令行执行该指令，将会提取视频`test.mp4`的帧图片，并将其保存至`frames`文件夹中，命名格式为6位有效数字+后缀名（如`000000.jpg`、`000001.jpg`）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video, dst):\n",
    "    '''\n",
    "    :param video: 视频的路径\n",
    "    :param dst: 存放输出帧图片的路径\n",
    "    :return: None\n",
    "    '''\n",
    "    ########## 请补全代码 ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 函数测试   \n",
    "如果书写的extract_frames函数正确，运行下面测试程序后，“test-folder/题1/frames”文件夹里的“测试文件.txt”将会被删除，同时该文件夹下将会出现从视频“test-folder/题1/test.mp4”中提取的帧图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cleanup: test-folder/题1/frames/\n"
     ]
    }
   ],
   "source": [
    "extract_frames('test-folder/题1/test.mp4', 'test-folder/题1/frames')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提取特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用resnet152提取视频特征，提取后的特征`.npy`文件保存在`output/train-video`中，每一个视频得到大小为`[40, 2048]` 的特征张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **题2：请补全提取视频特征的代码**  \n",
    "+ 该段代码要求实现的功能为：  \n",
    "    - 1.调用`extract_frames`函数，提取`video`的帧图片，图片保存至`dst`目录中；\n",
    "    - 2.在提取出来的图片中按名称顺序进行均匀采样，要求采样出`params['n_frame_steps']`张图片，并将这些图片保存到`image_list`中； \n",
    "    > - **提示**       \n",
    "    >（1）**采样方法**：使用`np.linspace()`进行采样。例如有编号0-9的10张图片，需要采样出8张图片：使用`np.linspace(0, 9, 8)`进行采样，得到`[0, 1.5, 3, 4.5, 6, 7.5, 9]`，再进行四舍五入（可使用`np.round()`），得到`[0, 2, 3, 4, 6, 8, 9]`    \n",
    "    > （2）**保存格式**：为方便读取，`image_list`中可保存采样后图片的路径。例如`dst`中有编号`0-9`的10张图片，采用（1）中的采样方式后，`image_list`中保存的内容应该为`[\"dst/0.jpg\", \"dst/2.jpg\", \"dst/3.jpg\", \"dst/4.jpg\", \"dst/6.jpg\", \"dst/8.jpg\", \"dst/9.jpg\"]`  \n",
    "    - 3.利用`load_image_fn()`方法将`image_list`中的所有图片转换成`tensor`格式，并保存至`images`中，这段代码完成后，`images`的维度应该为`(len(image_list), C, H, W)`；\n",
    "    >  - **`load_image_fn()`使用方法**  \n",
    "    >```python\n",
    "    img = 'dst/0.jpg'\n",
    "    image = load_image_fn(img) # load_image_fn会将图片转换成维度为(C, H, W)的tensor形式，C、H、W已在之前定义出\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feats(params, model, load_image_fn):\n",
    "    '''\n",
    "    :param params: 包含一系列可自行设置的参数\n",
    "    :param model: 提取特征所用的模型\n",
    "    :param load_image_fn: 读取图片并将图片转换成所用模型对应的格式\n",
    "    '''\n",
    "    global C, H, W\n",
    "    model.eval()\n",
    "\n",
    "    dir_fc = params['output_dir']    \n",
    "    if not os.path.isdir(dir_fc):\n",
    "        os.mkdir(dir_fc)\n",
    "    print(\"save video feats to %s\" % (dir_fc))\n",
    "    video_list = glob.glob(os.path.join(params['video_path'], '*.mp4'))  # 获取video_path下的所有.mp4文件\n",
    "    # print(video_list)\n",
    "    \n",
    "    for video in tqdm(video_list):  # tqdm：进度条\n",
    "        video_id = video.split(\"/\")[-1].split(\".\")[0]   \n",
    "        dst = params['model'] + '_' + video_id \n",
    "        \n",
    "        ########## 请补全代码 ##########\n",
    "           \n",
    "        with torch.no_grad(): \n",
    "            fc_feats = model(images).squeeze()\n",
    "       \n",
    "        img_feats = fc_feats.cpu().numpy()  # 用训练好的CNN模型，对每一个视频的图片提取特征  n_frame_steps*2048\n",
    "        outfile = os.path.join(dir_fc, video_id + '.npy')\n",
    "        outfile = outfile.replace('\\\\','/')\n",
    "        np.save(outfile, img_feats)  # 保存特征\n",
    "        shutil.rmtree(dst)           # 清除dst文件内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\"output_dir\":\"output\",'video_path':\"train-video\",'model':\"resnet152\",'n_frame_steps':40}\n",
    "\n",
    "model = pretrainedmodels.resnet152(pretrained='imagenet')\n",
    "load_image_fn = utils.LoadTransformImage(model)\n",
    "model.last_linear = utils.Identity()\n",
    "model = nn.DataParallel(model)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/74 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save video feats to output\n",
      " cleanup: resnet152_train-video\\G_00100/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 74/74 [01:26<00:00,  1.16s/it]\n"
     ]
    }
   ],
   "source": [
    "extract_feats(params,model,load_image_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.预处理——提取词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法介绍\n",
    "\n",
    "包括分词统计与词序列化两个部分\n",
    "\n",
    "分词统计：对视频的描述文字进行格式化的数据格式转换；\n",
    "\n",
    "- 首先对英文描述进行分词，将完整的句子拆分成单独的英文单词；\n",
    "\n",
    "- 在句子的开始添加\"sos\"标记，表明句子开始；句子结束为止添加\"eos\"标记，表明句子结束；\n",
    "    \n",
    "- 对所有英文单词进行词频统计，设置词频阈值，单词词频低于该阈值时，将该单词舍弃，并以“UNK”作为标记进行替代；\n",
    " \n",
    "词序列化：将所有单词进行索引化操作，即为单个单词建立索引，实现单词文本到数字序列之间的映射；\n",
    "\n",
    "- 建立“ix_to_word”对象，实现由索引提取单词的操作；\n",
    "\n",
    "- 建立“word_to_ix”对象，实现由单词转化为索引的操作；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法使用\n",
    "\n",
    "main(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数介绍\n",
    "\n",
    "input_json：视频信息与描述语句信息json文件路径\n",
    "\n",
    "caption_json:输出描述语句文件路径\n",
    "\n",
    "info_json：输出词汇文件路径\n",
    "\n",
    "word_count_threshold：阈值，词汇出现次数超过阈值会被记录；本次试验中词频阈值选择为1 \n",
    "\n",
    "bad_words：出现次数小于等于word_count_threshold的词\n",
    "\n",
    "total_words：总词数\n",
    "\n",
    "UNKs：Unknown Words，用来替代bad_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 命令实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **题3：请补全下面的`build_vocab()`函数代码**  \n",
    "+ **参数描述**  \n",
    "    `ws`是一条去除特殊符号的描述语句  \n",
    "    `counts`是一个字典，保存了各个单词出现的次数  \n",
    "    \n",
    "要求将`ws`转换成列表形式（每个单词作为一个元素），当单词出现次数小于等于阈值`count_thr`时，用`<UNK>`代替。然后在开头加上`<sos>`标记，在结尾加上`<eos>`标记，最后将转换后的描述语句保存到变量`caption`中。\n",
    "+ **示例**\n",
    "```python\n",
    "代码\n",
    "ws = \"A man wears a black tie\"\n",
    "count_thr = 1\n",
    "counts = {'A': 3, 'man': 2, 'wears': 2, 'a': 3, 'tie': 2}\n",
    "# 你的代码 #\n",
    "print('caption: ', caption)\n",
    "输出\n",
    "caption: ['<sos>', 'A', 'man', 'wears', 'a', '<UNK>', 'tie', '<eos>']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(vids, params):\n",
    "    count_thr = params['word_count_threshold']  \n",
    "\n",
    "    counts = {}\n",
    "    for vid, caps in vids.items():  \n",
    "        for cap in caps['captions']:\n",
    "            ws = re.sub(r'[.!,;?]', ' ', cap).split() # 忽略符号\n",
    "            \n",
    "            for w in ws:\n",
    "                counts[w] = counts.get(w, 0) + 1      #指有w时返回其值，默认是0，+1能够累计次数\n",
    "    # print(counts)  # 统计每个词对应的个数\n",
    "    total_words = sum(counts.values())       # 总词数\n",
    "    bad_words = [w for w, n in counts.items() if n <= count_thr] # 坏词\n",
    "    # print(bad_words)\n",
    "    vocab = [w for w, n in counts.items() if n > count_thr] # 好词\n",
    "    # print(vocab)\n",
    "    bad_count = sum(counts[w] for w in bad_words)  # 坏词数\n",
    "    print('number of bad words: %d/%d = %.2f%%' %\n",
    "          (len(bad_words), len(counts), len(bad_words) * 100.0 / len(counts)))\n",
    "    print('number of words in vocab would be %d' % (len(vocab), ))\n",
    "    print('number of UNKs: %d/%d = %.2f%%' %\n",
    "          (bad_count, total_words, bad_count * 100.0 / total_words))\n",
    "   \n",
    "    if bad_count > 0:\n",
    "        # 添加UNK，映射到坏词 ，例如['<sos>', 'A', 'man', 'wears', 'a', '<UNK>', 'tie', '<eos>']\n",
    "        print('inserting the special UNK token')\n",
    "        vocab.append('<UNK>')\n",
    "    for vid, caps in vids.items():\n",
    "        caps = caps['captions']\n",
    "        vids[vid]['final_captions'] = []\n",
    "        for cap in caps:\n",
    "            ws = re.sub(r'[.!,;?]', ' ', cap).split()\n",
    "            \n",
    "            ########## 请补全代码 ##########\n",
    "            \n",
    "            #print(caption)\n",
    "            vids[vid]['final_captions'].append(caption)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析输入json文件中的描述语句，将单词分为vocab和bad_words（出现次数小于word_count_threshold），并使用UNK替代bad_words。返回词汇表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **题4：请补全下面的`main()`函数代码**  \n",
    "`videos`的格式为:\n",
    "```\n",
    "{\n",
    "  \"caption\": \"A group of people are holding an international conference\",\n",
    "  \"sen_id\": 0,\n",
    "  \"video_id\": \"G_00100\"\n",
    "},\n",
    "{\n",
    "  \"sen_id\": 1,\n",
    "  \"video_id\": \"G_00100\",\n",
    "  \"caption\": \"There is a lot of water and flowers on the table\"\n",
    "},\n",
    "{\n",
    "  \"caption\": \"China and Panama Leaders'Meeting\",\n",
    "  \"sen_id\": 2,\n",
    "  \"video_id\": \"G_00100\"\n",
    "},\n",
    "{\n",
    "  \"caption\": \"A group of people are applauding\",\n",
    "  \"sen_id\": 0,\n",
    "  \"video_id\": \"G_00101\"\n",
    "},\n",
    "{\n",
    "  \"caption\": \"Xi Jinping Meeting with President of Panama\",\n",
    "  \"sen_id\": 1,\n",
    "  \"video_id\": \"G_00101\"\n",
    "},\n",
    "{\n",
    "  \"caption\": \"People attend the Economic and Trade Cooperation Forum\",\n",
    "  \"sen_id\": 2,\n",
    "  \"video_id\": \"G_00101\"\n",
    "}\n",
    "```\n",
    "请将其转换成以下格式，并保存到字典`video_caption`中:\n",
    "```\n",
    "{\n",
    "    \"G_00100\": \n",
    "        {\"captions\": \n",
    "            [\"A group of people are holding an international conference\", \n",
    "             \"There is a lot of water and flowers on the table\", \n",
    "             \"China and Panama Leaders'Meeting\"]\n",
    "        }, \n",
    "    \"G_00101\": \n",
    "        {\"captions\": \n",
    "            [\"A group of people are applauding\", \n",
    "             \"Xi Jinping Meeting with President of Panama\", \n",
    "             \"People attend the Economic and Trade Cooperation Forum\"]\n",
    "         }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(params):\n",
    "    videos = json.load(open(params['input_json'], 'r',encoding='UTF-8'))['sentences']\n",
    "    video_caption = {}\n",
    "    \n",
    "    ########## 请补全代码 ##########\n",
    "    \n",
    "    #print(video_caption)  #每一个视频对应多个描述\n",
    "    # 构建词汇表\n",
    "    vocab = build_vocab(video_caption, params)\n",
    "    itow = {i + 2: w for i, w in enumerate(vocab)}\n",
    "    wtoi = {w: i + 2 for i, w in enumerate(vocab)}  \n",
    "\n",
    "    wtoi['<eos>'] = 0\n",
    "    itow[0] = '<eos>'\n",
    "    wtoi['<sos>'] = 1\n",
    "    itow[1] = '<sos>'\n",
    "\n",
    "    out = {}\n",
    "    out['ix_to_word'] = itow\n",
    "    out['word_to_ix'] = wtoi\n",
    "    out['videos'] = {'train': [], 'val': [], 'test': []}\n",
    "    videos = json.load(open(params['input_json'], 'r',encoding='UTF-8'))['videos']\n",
    "    for i in videos:\n",
    "        out['videos'][i['split']].append(int(i['id']))\n",
    "    json.dump(out, open(params['info_json'], 'w'))\n",
    "    json.dump(video_caption, open(params['caption_json'], 'w'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\"word_count_threshold\":1,\"input_json\":\"data\\Video_info.json\",\"info_json\":\"data\\info.json\",\"caption_json\":\"data\\caption.json\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of bad words: 746/1310 = 56.95%\n",
      "number of words in vocab would be 564\n",
      "number of UNKs: 746/4709 = 15.84%\n",
      "inserting the special UNK token\n"
     ]
    }
   ],
   "source": [
    "main(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 词序列化结果保存在data文件夹中，命名为info.json；描述语句保存在caption.json中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 算法介绍\n",
    "\n",
    "![title](image1.png)\n",
    "\n",
    "\n",
    "模型使用使用两层 RNN，每个具有 512 个隐藏单元。第一个红色的 LSTM 层用于帧序列进行建模，输出隐状态作为第二层绿色 LSTM 层的输入用于对最终的输出词序列进行建模。\n",
    "\n",
    "训练：\n",
    "\n",
    "顶层 LSTM 接受帧序列并进行编码，第二层的 LSTM 接受第一层的隐状态 h, 并将其与零填充符相连后编码，该过程不计算损失值。\n",
    "\n",
    "在所有帧都输出隐状态后，第二层 LSTM 送入起始符<BOS>，使其开始将收到的隐状态解码成单词序列。\n",
    "    \n",
    "解码阶段进行训练时，在已知帧序列的隐状态及之前输出的单词的条件下，求预测句子的对数似然。训练目标就是使得下式得到最大值。\n",
    "\n",
    "![title](image6.png)\n",
    "\n",
    "\n",
    "整个训练数据集上使用随机梯度下降算法进行优化，从而使 LSTM 学习更合适的隐状态 h。\n",
    "\n",
    "第二层 LSTM 的输出 z 在词汇库 V 中寻找最大可能性的目标单词 y：\n",
    "\n",
    "![title](image5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法使用 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数介绍\n",
    "\n",
    "### Model settings:\n",
    "\n",
    "model: 选择适用的模型\n",
    "\n",
    "max_len：描述语句的最大长度\n",
    "\n",
    "bidirectional：0表示禁用，1表示启用。 编码器/解码器双向\n",
    "\n",
    "dim_hidden：RNN隐藏层的大小\n",
    "\n",
    "num_layers：RNN层数\n",
    "\n",
    "input_dropout_p：dropout强度\n",
    "\n",
    "rnn_type：RNN 类型，LSTM or GRU\n",
    "\n",
    "rnn_dropout：对 RNN模型的 dropout\n",
    "\n",
    "dim_word:词汇表中每个标记的编码大小\n",
    "\n",
    "### Data input settings:\n",
    "\n",
    "input_json: 包含视频信息的json文件的路径\n",
    "\n",
    "info_json: 包含其他信息和词汇的json文件的路径\n",
    "\n",
    "caption_json: 已处理的视频字幕json文件的路径\n",
    "\n",
    "feats_dir：包含预处理的fc特征的路径\n",
    "\n",
    "c3d_feats_dir：C3D特征路径\n",
    "\n",
    "with_c3d ：是否使用 C3D 特征\n",
    "\n",
    "cached_tokens：在训练期间计算cider分数的缓存令牌文件。\n",
    "\n",
    "### 选择参数：\n",
    "\n",
    "epochs：训练迭代次数\n",
    "\n",
    "batch_size： 输入数据批大小\n",
    "\n",
    "grad_clip：梯度阈值（解决梯度爆炸）\n",
    "\n",
    "self_crit_after：在哪个epoch后开始调整CNN，-1 禁用; 0 从开始就进行微调\n",
    "\n",
    "dim_vid: 输入特征维数\n",
    "\n",
    "learning_rate：学习速率\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 命令实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import misc.utils as utils\n",
    "import opts\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_value_\n",
    "from dataloader import VideoDataset\n",
    "from misc.rewards import get_self_critical_reward, init_cider_scorer\n",
    "from models import DecoderRNN, EncoderRNN, S2VTAttModel, S2VTModel\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = {\"input_json\":\"data\\\\Video_info.json\",\n",
    "       \"info_json\":\"data\\\\info.json\",\n",
    "       \"caption_json\":\"data\\\\caption.json\",\n",
    "       \"feats_dir\":\"output\\\\train-video\",\n",
    "       \"c3d_feats_dir\":\"\",\n",
    "       \"with_c3d\":0,\n",
    "       \"cached_tokens\":\"msr-all-idxs\",\n",
    "       \"model\":\"S2VTModel\",\n",
    "       \"max_len\":28,\n",
    "       \"bidirectional\": 0,\n",
    "       \"dim_hidden\":512 ,\n",
    "       \"num_layers\":1 ,\n",
    "       \"input_dropout_p\":0.2 ,\n",
    "       \"rnn_type\":\"gru\",\n",
    "       \"rnn_dropout_p\":0.5,\n",
    "       \"dim_word\":512,\n",
    "       \"dim_vid\":2048,\n",
    "       \"epochs\":301,\n",
    "       \"batch_size\":1,\n",
    "       \"grad_clip\":5,\n",
    "       \"self_crit_after\":-1,\n",
    "       \"learning_rate\":4e-4,\n",
    "       \"learning_rate_decay_rate\":0.8,\n",
    "       \"learning_rate_decay_every\":200,\n",
    "       \"optim_alpha\":0.9,\n",
    "       \"optim_beta\":0.999,\n",
    "       \"optim_epsilon\":1e-8,\n",
    "       \"weight_decay\":5e-4,\n",
    "       \"save_checkpoint_every\":50,\n",
    "       \"checkpoint_path\":\"save\",\n",
    "       \"gpu\":0\n",
    "      }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （1）准备数据:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save opt details to save\\opt_info.json\n"
     ]
    }
   ],
   "source": [
    "opt_json = os.path.join(opt[\"checkpoint_path\"], 'opt_info.json')\n",
    "if not os.path.isdir(opt[\"checkpoint_path\"]):\n",
    "    os.mkdir(opt[\"checkpoint_path\"])\n",
    "with open(opt_json, 'w') as f:\n",
    "    json.dump(opt, f)\n",
    "print('save opt details to %s' % (opt_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size is  567\n",
      "number of train videos:  70\n",
      "number of val videos:  0\n",
      "number of test videos:  4\n",
      "load feats from output\\train-video\n",
      "max sequence length in data is 28\n"
     ]
    }
   ],
   "source": [
    "dataset = VideoDataset(opt, 'train')  \n",
    "dataloader = DataLoader(dataset, batch_size=opt[\"batch_size\"], shuffle=True) # DataLoader将数据根据batch size大小、是否shuffle等封装成一个Batch Size大小的Tensor，用于后面的训练。shuffle将元素随机排列\n",
    "opt[\"vocab_size\"] = dataset.get_vocab_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （2）定义网络结构:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **题5：请补全下面构造S2VT模型的代码**  \n",
    "请查看`models/S2VTModel.py`路径的[S2VTModel源码](models/S2VTModel.py)，根据`opt`中的参数构造`S2VT`模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt[\"model\"] == 'S2VTModel':\n",
    "        model = ########## 请补全代码 ##########\n",
    "elif opt[\"model\"] == \"S2VTAttModel\":\n",
    "        encoder = EncoderRNN(\n",
    "            opt[\"dim_vid\"],\n",
    "            opt[\"dim_hidden\"],\n",
    "            bidirectional=opt[\"bidirectional\"],\n",
    "            input_dropout_p=opt[\"input_dropout_p\"],\n",
    "            rnn_cell=opt['rnn_type'],\n",
    "            rnn_dropout_p=opt[\"rnn_dropout_p\"])\n",
    "        decoder = DecoderRNN(\n",
    "            opt[\"vocab_size\"],\n",
    "            opt[\"max_len\"],\n",
    "            opt[\"dim_hidden\"],\n",
    "            opt[\"dim_word\"],\n",
    "            input_dropout_p=opt[\"input_dropout_p\"],\n",
    "            rnn_cell=opt['rnn_type'],\n",
    "            rnn_dropout_p=opt[\"rnn_dropout_p\"],\n",
    "            bidirectional=opt[\"bidirectional\"])       \n",
    "        model = S2VTAttModel(encoder, decoder)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  （3）定义损失函数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "crit = utils.LanguageModelCriterion() \n",
    "rl_crit = utils.RewardCriterion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （4）定义迭代优化算法：Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(         \n",
    "    model.parameters(),    #将网络参数放到优化器里    \n",
    "    lr=opt[\"learning_rate\"],   \n",
    "    weight_decay=opt[\"weight_decay\"]) \n",
    "\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(       \n",
    "    optimizer, \n",
    "    step_size=opt[\"learning_rate_decay_every\"], \n",
    "    gamma=opt[\"learning_rate_decay_rate\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （5）迭代训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S2VTModel(\n",
       "  (rnn1): GRU(2048, 512, batch_first=True, dropout=0.5)\n",
       "  (rnn2): GRU(1024, 512, batch_first=True, dropout=0.5)\n",
       "  (embedding): Embedding(567, 512)\n",
       "  (out): Linear(in_features=512, out_features=567, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 (epoch 0), train_loss = 24.901928\n",
      "iter 2 (epoch 0), train_loss = 23.424854\n",
      "iter 3 (epoch 0), train_loss = 63.177578\n",
      "iter 4 (epoch 0), train_loss = 57.602665\n",
      "iter 5 (epoch 0), train_loss = 63.253719\n",
      "iter 6 (epoch 0), train_loss = 43.669899\n",
      "iter 7 (epoch 0), train_loss = 42.917816\n",
      "iter 8 (epoch 0), train_loss = 38.557159\n",
      "iter 9 (epoch 0), train_loss = 21.914635\n",
      "iter 10 (epoch 0), train_loss = 61.325832\n",
      "iter 11 (epoch 0), train_loss = 58.056610\n",
      "iter 12 (epoch 0), train_loss = 42.238190\n",
      "iter 13 (epoch 0), train_loss = 26.240246\n",
      "iter 14 (epoch 0), train_loss = 38.005875\n",
      "iter 15 (epoch 0), train_loss = 23.372185\n",
      "iter 16 (epoch 0), train_loss = 29.681520\n",
      "iter 17 (epoch 0), train_loss = 27.285995\n",
      "iter 18 (epoch 0), train_loss = 40.775490\n",
      "iter 19 (epoch 0), train_loss = 33.239227\n",
      "iter 20 (epoch 0), train_loss = 29.534119\n",
      "iter 21 (epoch 0), train_loss = 62.466633\n",
      "iter 22 (epoch 0), train_loss = 39.408524\n",
      "iter 23 (epoch 0), train_loss = 19.127930\n",
      "iter 24 (epoch 0), train_loss = 23.667763\n",
      "iter 25 (epoch 0), train_loss = 41.712742\n",
      "iter 26 (epoch 0), train_loss = 5.918874\n",
      "iter 27 (epoch 0), train_loss = 33.241280\n",
      "iter 28 (epoch 0), train_loss = 37.469563\n",
      "iter 29 (epoch 0), train_loss = 24.859592\n",
      "iter 30 (epoch 0), train_loss = 20.929115\n",
      "iter 31 (epoch 0), train_loss = 23.958736\n",
      "iter 32 (epoch 0), train_loss = 84.231766\n",
      "iter 33 (epoch 0), train_loss = 25.682428\n",
      "iter 34 (epoch 0), train_loss = 46.881336\n",
      "iter 35 (epoch 0), train_loss = 16.120911\n",
      "iter 36 (epoch 0), train_loss = 31.750187\n",
      "iter 37 (epoch 0), train_loss = 31.881014\n",
      "iter 38 (epoch 0), train_loss = 43.287189\n",
      "iter 39 (epoch 0), train_loss = 33.033539\n",
      "iter 40 (epoch 0), train_loss = 25.801933\n",
      "iter 41 (epoch 0), train_loss = 82.168610\n",
      "iter 42 (epoch 0), train_loss = 32.323997\n",
      "iter 43 (epoch 0), train_loss = 20.379251\n",
      "iter 44 (epoch 0), train_loss = 36.023746\n",
      "iter 45 (epoch 0), train_loss = 25.434273\n",
      "iter 46 (epoch 0), train_loss = 62.227795\n",
      "iter 47 (epoch 0), train_loss = 12.406312\n",
      "iter 48 (epoch 0), train_loss = 36.019012\n",
      "iter 49 (epoch 0), train_loss = 36.520695\n",
      "iter 50 (epoch 0), train_loss = 36.536407\n",
      "iter 51 (epoch 0), train_loss = 55.822899\n",
      "iter 52 (epoch 0), train_loss = 37.374504\n",
      "iter 53 (epoch 0), train_loss = 36.408085\n",
      "iter 54 (epoch 0), train_loss = 54.666740\n",
      "iter 55 (epoch 0), train_loss = 33.910934\n",
      "iter 56 (epoch 0), train_loss = 21.794724\n",
      "iter 57 (epoch 0), train_loss = 33.964809\n",
      "iter 58 (epoch 0), train_loss = 32.132050\n",
      "iter 59 (epoch 0), train_loss = 40.085373\n",
      "iter 60 (epoch 0), train_loss = 51.064865\n",
      "iter 61 (epoch 0), train_loss = 44.138405\n",
      "iter 62 (epoch 0), train_loss = 66.378113\n",
      "iter 63 (epoch 0), train_loss = 33.098934\n",
      "iter 64 (epoch 0), train_loss = 15.734628\n",
      "iter 65 (epoch 0), train_loss = 47.099712\n",
      "iter 66 (epoch 0), train_loss = 32.787735\n",
      "iter 67 (epoch 0), train_loss = 38.727051\n",
      "iter 68 (epoch 0), train_loss = 21.293171\n",
      "iter 69 (epoch 0), train_loss = 26.953485\n",
      "iter 70 (epoch 0), train_loss = 37.749153\n",
      "model saved to save\\model_0.pth\n",
      "iter 1 (epoch 1), train_loss = 42.894936\n",
      "iter 2 (epoch 1), train_loss = 22.299183\n",
      "iter 3 (epoch 1), train_loss = 39.995602\n",
      "iter 4 (epoch 1), train_loss = 53.053413\n",
      "iter 5 (epoch 1), train_loss = 22.589611\n",
      "iter 6 (epoch 1), train_loss = 30.087166\n",
      "iter 7 (epoch 1), train_loss = 11.331169\n",
      "iter 8 (epoch 1), train_loss = 38.656368\n",
      "iter 9 (epoch 1), train_loss = 34.132687\n",
      "iter 10 (epoch 1), train_loss = 31.252375\n",
      "iter 11 (epoch 1), train_loss = 29.066315\n",
      "iter 12 (epoch 1), train_loss = 23.495804\n",
      "iter 13 (epoch 1), train_loss = 36.602646\n",
      "iter 14 (epoch 1), train_loss = 21.638811\n",
      "iter 15 (epoch 1), train_loss = 31.484354\n",
      "iter 16 (epoch 1), train_loss = 68.462334\n",
      "iter 17 (epoch 1), train_loss = 22.488132\n",
      "iter 18 (epoch 1), train_loss = 26.638424\n",
      "iter 19 (epoch 1), train_loss = 69.425995\n",
      "iter 20 (epoch 1), train_loss = 33.983295\n",
      "iter 21 (epoch 1), train_loss = 13.523117\n",
      "iter 22 (epoch 1), train_loss = 6.507699\n",
      "iter 23 (epoch 1), train_loss = 46.143887\n",
      "iter 24 (epoch 1), train_loss = 35.173351\n",
      "iter 25 (epoch 1), train_loss = 15.873886\n",
      "iter 26 (epoch 1), train_loss = 18.731926\n",
      "iter 27 (epoch 1), train_loss = 15.115744\n",
      "iter 28 (epoch 1), train_loss = 35.967361\n",
      "iter 29 (epoch 1), train_loss = 18.545942\n",
      "iter 30 (epoch 1), train_loss = 25.029675\n",
      "iter 31 (epoch 1), train_loss = 35.819019\n",
      "iter 32 (epoch 1), train_loss = 29.941771\n",
      "iter 33 (epoch 1), train_loss = 25.402714\n",
      "iter 34 (epoch 1), train_loss = 45.185349\n",
      "iter 35 (epoch 1), train_loss = 22.921698\n",
      "iter 36 (epoch 1), train_loss = 25.362823\n",
      "iter 37 (epoch 1), train_loss = 47.724792\n",
      "iter 38 (epoch 1), train_loss = 30.140118\n",
      "iter 39 (epoch 1), train_loss = 38.362984\n",
      "iter 40 (epoch 1), train_loss = 26.890411\n",
      "iter 41 (epoch 1), train_loss = 26.618816\n",
      "iter 42 (epoch 1), train_loss = 48.686981\n",
      "iter 43 (epoch 1), train_loss = 35.799423\n",
      "iter 44 (epoch 1), train_loss = 44.018547\n",
      "iter 45 (epoch 1), train_loss = 68.244446\n",
      "iter 46 (epoch 1), train_loss = 6.648610\n",
      "iter 47 (epoch 1), train_loss = 47.018169\n",
      "iter 48 (epoch 1), train_loss = 32.772457\n",
      "iter 49 (epoch 1), train_loss = 31.611597\n",
      "iter 50 (epoch 1), train_loss = 29.998316\n",
      "iter 51 (epoch 1), train_loss = 47.657276\n",
      "iter 52 (epoch 1), train_loss = 33.246651\n",
      "iter 53 (epoch 1), train_loss = 34.500328\n",
      "iter 54 (epoch 1), train_loss = 18.180132\n",
      "iter 55 (epoch 1), train_loss = 26.773613\n",
      "iter 56 (epoch 1), train_loss = 28.844305\n",
      "iter 57 (epoch 1), train_loss = 26.856331\n",
      "iter 58 (epoch 1), train_loss = 37.883667\n",
      "iter 59 (epoch 1), train_loss = 36.323906\n",
      "iter 60 (epoch 1), train_loss = 6.047636\n",
      "iter 61 (epoch 1), train_loss = 38.033127\n",
      "iter 62 (epoch 1), train_loss = 28.254337\n",
      "iter 63 (epoch 1), train_loss = 34.032654\n",
      "iter 64 (epoch 1), train_loss = 84.291992\n",
      "iter 65 (epoch 1), train_loss = 35.315174\n",
      "iter 66 (epoch 1), train_loss = 23.185743\n",
      "iter 67 (epoch 1), train_loss = 27.898054\n",
      "iter 68 (epoch 1), train_loss = 23.730846\n",
      "iter 69 (epoch 1), train_loss = 24.696058\n",
      "iter 70 (epoch 1), train_loss = 20.625881\n",
      "iter 1 (epoch 2), train_loss = 36.540058\n",
      "iter 2 (epoch 2), train_loss = 23.006311\n",
      "iter 3 (epoch 2), train_loss = 25.887218\n",
      "iter 4 (epoch 2), train_loss = 21.626850\n",
      "iter 5 (epoch 2), train_loss = 18.359634\n",
      "iter 6 (epoch 2), train_loss = 21.033089\n",
      "iter 7 (epoch 2), train_loss = 27.194794\n",
      "iter 8 (epoch 2), train_loss = 19.879715\n",
      "iter 9 (epoch 2), train_loss = 51.508717\n",
      "iter 10 (epoch 2), train_loss = 14.330648\n",
      "iter 11 (epoch 2), train_loss = 16.159990\n",
      "iter 12 (epoch 2), train_loss = 30.392044\n",
      "iter 13 (epoch 2), train_loss = 44.118294\n",
      "iter 14 (epoch 2), train_loss = 14.941160\n",
      "iter 15 (epoch 2), train_loss = 28.870876\n",
      "iter 16 (epoch 2), train_loss = 35.156704\n",
      "iter 17 (epoch 2), train_loss = 11.570917\n",
      "iter 18 (epoch 2), train_loss = 27.355030\n",
      "iter 19 (epoch 2), train_loss = 34.682964\n",
      "iter 20 (epoch 2), train_loss = 22.184288\n",
      "iter 21 (epoch 2), train_loss = 18.668798\n",
      "iter 22 (epoch 2), train_loss = 28.246586\n",
      "iter 23 (epoch 2), train_loss = 19.955547\n",
      "iter 24 (epoch 2), train_loss = 18.000614\n",
      "iter 25 (epoch 2), train_loss = 34.002041\n",
      "iter 26 (epoch 2), train_loss = 39.854813\n",
      "iter 27 (epoch 2), train_loss = 35.929661\n",
      "iter 28 (epoch 2), train_loss = 21.623756\n",
      "iter 29 (epoch 2), train_loss = 23.765038\n",
      "iter 30 (epoch 2), train_loss = 30.503107\n",
      "iter 31 (epoch 2), train_loss = 32.304874\n",
      "iter 32 (epoch 2), train_loss = 36.512398\n",
      "iter 33 (epoch 2), train_loss = 18.534599\n",
      "iter 34 (epoch 2), train_loss = 19.180792\n",
      "iter 35 (epoch 2), train_loss = 52.791260\n",
      "iter 36 (epoch 2), train_loss = 15.692657\n",
      "iter 37 (epoch 2), train_loss = 16.437944\n",
      "iter 38 (epoch 2), train_loss = 32.175594\n",
      "iter 39 (epoch 2), train_loss = 29.325384\n",
      "iter 40 (epoch 2), train_loss = 18.132017\n",
      "iter 41 (epoch 2), train_loss = 27.103350\n",
      "iter 42 (epoch 2), train_loss = 34.406410\n",
      "iter 43 (epoch 2), train_loss = 39.728260\n",
      "iter 44 (epoch 2), train_loss = 25.761475\n",
      "iter 45 (epoch 2), train_loss = 22.291885\n",
      "iter 46 (epoch 2), train_loss = 36.733582\n",
      "iter 47 (epoch 2), train_loss = 26.361845\n",
      "iter 48 (epoch 2), train_loss = 18.411057\n",
      "iter 49 (epoch 2), train_loss = 54.100403\n",
      "iter 50 (epoch 2), train_loss = 88.941139\n",
      "iter 51 (epoch 2), train_loss = 23.224297\n",
      "iter 52 (epoch 2), train_loss = 64.269722\n",
      "iter 53 (epoch 2), train_loss = 20.291569\n",
      "iter 54 (epoch 2), train_loss = 36.727791\n",
      "iter 55 (epoch 2), train_loss = 19.316866\n",
      "iter 56 (epoch 2), train_loss = 18.027699\n",
      "iter 57 (epoch 2), train_loss = 26.081917\n",
      "iter 58 (epoch 2), train_loss = 19.399601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 59 (epoch 2), train_loss = 27.294922\n",
      "iter 60 (epoch 2), train_loss = 18.611269\n",
      "iter 61 (epoch 2), train_loss = 34.032806\n",
      "iter 62 (epoch 2), train_loss = 48.014523\n",
      "iter 63 (epoch 2), train_loss = 29.490538\n",
      "iter 64 (epoch 2), train_loss = 28.841024\n",
      "iter 65 (epoch 2), train_loss = 35.977760\n",
      "iter 66 (epoch 2), train_loss = 45.652275\n",
      "iter 67 (epoch 2), train_loss = 24.517471\n",
      "iter 68 (epoch 2), train_loss = 29.430014\n",
      "iter 69 (epoch 2), train_loss = 21.589661\n",
      "iter 70 (epoch 2), train_loss = 27.370144\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(opt[\"epochs\"]): \n",
    "    exp_lr_scheduler.step() \n",
    "    iteration = 0\n",
    "\n",
    "    if opt[\"self_crit_after\"] != -1 and epoch >= opt[\"self_crit_after\"]:  \n",
    "        sc_flag = True \n",
    "        init_cider_scorer(opt[\"cached_tokens\"]) \n",
    "    else:\n",
    "        sc_flag = False\n",
    "        \n",
    "    # 准备训练数据：    \n",
    "    \n",
    "    for data in dataloader: \n",
    "        torch.cuda.synchronize() \n",
    "        fc_feats = data['fc_feats'].cuda()\n",
    "        labels = data['labels'].cuda()\n",
    "        masks = data['masks'].cuda()\n",
    "        fc_feats = data['fc_feats'].cuda()\n",
    "        labels = data['labels'].cuda()\n",
    "        masks = data['masks'].cuda()\n",
    "            \n",
    "        optimizer.zero_grad()  #清空优化器里的梯度信息\n",
    "        if not sc_flag:\n",
    "            seq_probs, _ = model(fc_feats, labels, 'train')      # 正向传播\n",
    "            loss = crit(seq_probs, labels[:, 1:], masks[:, 1:])  # 计算损失\n",
    "        else:\n",
    "            seq_probs, seq_preds = model(\n",
    "                fc_feats, mode='inference', opt=opt)\n",
    "            reward = get_self_critical_reward(model, fc_feats, data,\n",
    "                                                  seq_preds)\n",
    "            print(reward.shape)\n",
    "            loss = rl_crit(seq_probs, seq_preds,\n",
    "                            torch.from_numpy(reward).float().cuda())\n",
    "            \n",
    "        loss.backward()       #反向传播\n",
    "        clip_grad_value_(model.parameters(), opt['grad_clip'])\n",
    "            \n",
    "        optimizer.step()     # 更新参数\n",
    "        train_loss = loss.item()\n",
    "        torch.cuda.synchronize()\n",
    "        iteration += 1\n",
    "            \n",
    "        if not sc_flag:\n",
    "            print(\"iter %d (epoch %d), train_loss = %.6f\" %\n",
    "                    (iteration, epoch, train_loss))\n",
    "        else:\n",
    "            print(\"iter %d (epoch %d), avg_reward = %.6f\" %\n",
    "                    (iteration, epoch, np.mean(reward[:, 0])))\n",
    "                \n",
    "    if epoch % opt[\"save_checkpoint_every\"] == 0:\n",
    "        model_path = os.path.join(opt[\"checkpoint_path\"],\n",
    "                                      'model_%d.pth' % (epoch))\n",
    "        model_info_path = os.path.join(opt[\"checkpoint_path\"],\n",
    "                                           'model_score.txt')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(\"model saved to %s\" % (model_path))\n",
    "        with open(model_info_path, 'a') as f:\n",
    "            f.write(\"model_%d, loss: %.6f\\n\" % (epoch, train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 模型以及对应损失值保存在save文件夹中 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法介绍\n",
    "\n",
    "将测试视频传入已经训练好的模型中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数介绍\n",
    "\n",
    "recover_opt：原始参数文件所在的路径\n",
    "\n",
    "saved_model：测试模型所在的路径\n",
    "\n",
    "batch_size：输入数据批大小\n",
    "\n",
    "sample_max：是否在推测阶段采样最大概率以获取下一个单词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 命令实例 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from models import EncoderRNN, DecoderRNN, S2VTAttModel, S2VTModel\n",
    "from dataloader import VideoDataset\n",
    "import misc.utils as utils\n",
    "from misc.cocoeval import suppress_stdout_stderr, COCOScorer\n",
    "from pandas.io.json import json_normalize\n",
    "import nltk.tokenize as tk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转换数据格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_coco_scorer_format(data_frame):\n",
    "    gts = {}\n",
    "    for row in zip(data_frame[\"caption\"], data_frame[\"video_id\"]):\n",
    "        if row[1] in gts:\n",
    "            gts[row[1]].append(\n",
    "                {'image_id': row[1], 'cap_id': len(gts[row[1]]), 'caption': row[0]})\n",
    "        else:\n",
    "            gts[row[1]] = []\n",
    "            gts[row[1]].append(\n",
    "                {'image_id': row[1], 'cap_id': len(gts[row[1]]), 'caption': row[0]})\n",
    "    return gts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试，产生预测句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, crit, dataset, vocab, opt):\n",
    "    model.eval()\n",
    "    loader = DataLoader(dataset, batch_size=opt[\"batch_size\"], shuffle=True)\n",
    "    gt_dataframe = json_normalize(\n",
    "        json.load(open(opt[\"input_json\"],'r', encoding='UTF-8'))['sentences'])\n",
    "    gts = convert_data_to_coco_scorer_format(gt_dataframe)\n",
    "    \n",
    "    results = []\n",
    "    samples = {}\n",
    "    \n",
    "    for data in loader:\n",
    "        \n",
    "        fc_feats = data['fc_feats'].cuda()\n",
    "        labels = data['labels'].cuda()\n",
    "        masks = data['masks'].cuda()\n",
    "        video_ids = data['video_ids']\n",
    "      \n",
    "        with torch.no_grad():\n",
    "            seq_probs, seq_preds = model(\n",
    "                fc_feats, mode='inference', opt=opt)\n",
    "\n",
    "        sents = utils.decode_sequence(vocab, seq_preds)\n",
    "       \n",
    "        for k, sent in enumerate(sents):\n",
    "            video_id = video_ids[k]\n",
    "            samples[video_id] = [{'image_id': video_id, 'caption': sent}]\n",
    "    if not os.path.exists(opt[\"results_path\"]):\n",
    "        os.makedirs(opt[\"results_path\"])\n",
    "        \n",
    "    with open(os.path.join(opt[\"results_path\"],\n",
    "                           opt[\"model\"].split(\"/\")[-1].split('.')[0] + \".json\"), 'w') as prediction_results:\n",
    "        json.dump({\"predictions\": samples},\n",
    "                  prediction_results)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(opt):\n",
    "    dataset = VideoDataset(opt, \"test\")\n",
    "    opt[\"vocab_size\"] = dataset.get_vocab_size()\n",
    "    opt[\"seq_length\"] = dataset.max_len\n",
    "    if opt[\"model\"] == 'S2VTModel':\n",
    "        model = S2VTModel(opt[\"vocab_size\"], opt[\"max_len\"], opt[\"dim_hidden\"], opt[\"dim_word\"],\n",
    "                          rnn_dropout_p=opt[\"rnn_dropout_p\"]).cuda()\n",
    "        \n",
    "    elif opt[\"model\"] == \"S2VTAttModel\":\n",
    "        encoder = EncoderRNN(opt[\"dim_vid\"], opt[\"dim_hidden\"], bidirectional=opt[\"bidirectional\"],\n",
    "                             input_dropout_p=opt[\"input_dropout_p\"], rnn_dropout_p=opt[\"rnn_dropout_p\"])\n",
    "        decoder = DecoderRNN(opt[\"vocab_size\"], opt[\"max_len\"], opt[\"dim_hidden\"], opt[\"dim_word\"],\n",
    "                             input_dropout_p=opt[\"input_dropout_p\"],\n",
    "                             rnn_dropout_p=opt[\"rnn_dropout_p\"], bidirectional=opt[\"bidirectional\"])\n",
    "        model = S2VTAttModel(encoder, decoder).cuda()\n",
    "        \n",
    "    model.load_state_dict(torch.load(opt[\"saved_model\"]))\n",
    "    crit = utils.LanguageModelCriterion()\n",
    "\n",
    "    test(model, crit, dataset, dataset.get_vocab(), opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "args={\"recover_opt\":\"save/opt_info.json\",\"saved_model\":\"save/model_300.pth\",\"dump_json\":1,\"results_path\":\"results\",\"dump_path\":0,\"gpu\":0,\"batch_size\":1,\"sample_max\":1,\"temperature\":1,\"beam_size\":1}\n",
    "opt = json.load(open(args[\"recover_opt\"]))\n",
    "for k, v in args.items():\n",
    "    opt[k] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size is  567\n",
      "number of train videos:  70\n",
      "number of val videos:  0\n",
      "number of test videos:  4\n",
      "load feats from output\\train-video\n",
      "max sequence length in data is 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "D:\\software\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "<ipython-input-11-27652dfd9a2d>:4: FutureWarning: pandas.io.json.json_normalize is deprecated, use pandas.json_normalize instead\n",
      "  gt_dataframe = json_normalize(\n"
     ]
    }
   ],
   "source": [
    "main(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 预测结果储存为json文件，保存在`results`文件夹中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 部分结果展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import shutil\n",
    "import os\n",
    "import cv2\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def show_video(video, dst):\n",
    "    with open(os.devnull, \"w\") as ffmpeg_log:\n",
    "        if os.path.exists(dst):\n",
    "            shutil.rmtree(dst)                        \n",
    "        os.makedirs(dst)       \n",
    "        video_to_frames_command = [\"ffmpeg\",\n",
    "                                   '-y',               \n",
    "                                   '-i', video,        \n",
    "                                   '-vf', \"scale=400:300\",  \n",
    "                                   '-qscale:v', \"2\",   \n",
    "                                   '{0}/%06d.jpg'.format(dst)]\n",
    "        subprocess.call(video_to_frames_command,\n",
    "                        stdout=ffmpeg_log, stderr=ffmpeg_log)\n",
    "        imgbox = widgets.Image(format='jpg', height=300, width=400)\n",
    "        display(imgbox)\n",
    "        root = dst\n",
    "        for file in os.listdir(root):\n",
    "            canvas = cv2.imread(os.path.join(root, file))\n",
    "            imgbox.value = cv2.imencode('.jpg', canvas)[1].tobytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bebce74443f543358d59e83bad6dc52f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', format='jpg', height='300', width='400')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_video(\"train-video/G_00173.mp4\",\"frame_save/173\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型预测描述： The goods from the overturned truck were scattered all over the place\n",
    "                        （倾倒卡车的货物散落到各处）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ef79badd4f470a9572155519dd0230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', format='jpg', height='300', width='400')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_video(\"train-video/G_00171.mp4\",\"frame_save/171\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型预测描述： A white van pulled into a door\n",
    "                        （一辆白色货车驶入门）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 注： 若使用更多的视频来训练模型，效果会更好！ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
